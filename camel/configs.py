import os
from io import TextIOWrapper
from json import JSONDecodeError
from typing import Dict, Optional, Sequence, Union, Set, Type, Literal
import json
import logging

from pydantic import BaseSettings, Field, root_validator
from pydantic.class_validators import _FUNCS

from camel.typing import ModelType, InteractionMode

_FUNCS.clear()
_logger = logging.getLogger(__name__)


class BaseModelConfig(BaseSettings):
    model_type: ModelType
    interaction_mode: InteractionMode = InteractionMode.AUTO
    chat_session: str | None = None

    class Config:
        env_file = ".env"


class OpenAIModelConfig(BaseModelConfig):
    r"""Defines the parameters for generating chat completions using the
    OpenAI API.

    Args:
        temperature (float, optional): Sampling temperature to use, between
            :obj:`0` and :obj:`2`. Higher values make the output more random,
            while lower values make it more focused and deterministic.
            (default: :obj:`0.2`)
        top_p (float, optional): An alternative to sampling with temperature,
            called nucleus sampling, where the model considers the results of
            the tokens with top_p probability mass. So :obj:`0.1` means only
            the tokens comprising the top 10% probability mass are considered.
            (default: :obj:`1.0`)
        n (int, optional): How many chat completion choices to generate for
            each input message. ()default: :obj:`1`)
        stream (bool, optional): If True, partial message deltas will be sent
            as data-only server-sent events as they become available.
            (default: :obj:`False`)
        stop (str or list, optional): Up to :obj:`4` sequences where the API
            will stop generating further tokens. (default: :obj:`None`)
        max_tokens (int, optional): The maximum number of tokens to generate
            in the chat completion. The total length of input tokens and
            generated tokens is limited by the model's context length.
            (default: :obj:`None`)
        presence_penalty (float, optional): Number between :obj:`-2.0` and
            :obj:`2.0`. Positive values penalize new tokens based on whether
            they appear in the text so far, increasing the model's likelihood
            to talk about new topics. See more information about frequency and
            presence penalties. (default: :obj:`0.0`)
        frequency_penalty (float, optional): Number between :obj:`-2.0` and
            :obj:`2.0`. Positive values penalize new tokens based on their
            existing frequency in the text so far, decreasing the model's
            likelihood to repeat the same line verbatim. See more information
            about frequency and presence penalties. (default: :obj:`0.0`)
        logit_bias (dict, optional): Modify the likelihood of specified tokens
            appearing in the completion. Accepts a json object that maps tokens
            (specified by their token ID in the tokenizer) to an associated
            bias value from :obj:`-100` to :obj:`100`. Mathematically, the bias
            is added to the logits generated by the model prior to sampling.
            The exact effect will vary per model, but values between:obj:` -1`
            and :obj:`1` should decrease or increase likelihood of selection;
            values like :obj:`-100` or :obj:`100` should result in a ban or
            exclusive selection of the relevant token. (default: :obj:`{}`)
        user (str, optional): A unique identifier representing your end-user,
            which can help OpenAI to monitor and detect abuse.
            (default: :obj:`""`)
    """
    model_type: ModelType = ModelType.GPT_3_5_TURBO
    chat_token: str = Field(..., env='CHATGPT_TOKEN')
    sub_model: Literal['gpt4', 'default'] = 'default'

    temperature: float = 0.2  # openai default: 1.0
    top_p: float = 1.0
    n: int = 1
    stream: bool = False
    stop: Optional[Union[str, Sequence[str]]] = None
    max_tokens: Optional[int] = None
    presence_penalty: float = 0.0
    frequency_penalty: float = 0.0
    logit_bias: Dict = {}
    user: str = ""

    @root_validator
    def auto_set(cls, values):
        if values['model_type'] in (ModelType.GPT_4, ModelType.GPT_4_32k):
            values['sub_model'] = 'gpt4'
        return values


class GoogleBardModelConfig(BaseModelConfig):
    model_type = ModelType.GOOGLE_BARD
    chat_token: str = Field(..., env='BARDCHAT_TOKEN')


class BingModelConfig(BaseModelConfig):
    model_type = ModelType.BING_CHAT
    cookie_path: str = "/data/cookiesBing.json"
    upload_path: str = "/data/uploaded/cookiesBing.json"

    def update_cookie(self, cookie_file) -> bool:
        try:
            # Convert the BytesIO to a TextIOWrapper (which provides a file-like API) and load the JSON
            uploaded_file_wrapper = TextIOWrapper(cookie_file, encoding='utf-8')
            data = json.load(uploaded_file_wrapper)

            # Save the JSON data to the specified path
            with open(self.upload_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
            return True
        except Exception as e:
            _logger.error(str(e))
            return False

    def validate_cookie_path(self) -> str:
        if os.path.exists(self.upload_path):
            cookie_path = self.upload_path
        elif os.path.exists(self.cookie_path):
            cookie_path = self.cookie_path
        elif os.path.exists("cookiesBing.json"):
            cookie_path = "cookiesBing.json"
        else:
            raise ValueError("File 'cookiesBing.json' not found! Create it and put your cookies in there in the JSON "
                          "format.")
        with open(cookie_path, "r") as file:
            try:
                json.loads(file.read())
                return cookie_path
            except JSONDecodeError:
                raise ValueError(
                    "You did not put your cookies inside 'cookiesBing.json'! You can find the simple guide to get the "
                    "cookie file here: "
                    "https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required."
                )


class ModelConfigFactory:
    _instances: Dict[ModelType, BaseModelConfig] = {}
    _config_class_mapping: Dict[ModelType, Type[BaseModelConfig]] = {
        ModelType.GPT_3_5_TURBO: OpenAIModelConfig,
        ModelType.GPT_4: OpenAIModelConfig,
        ModelType.GPT_4_32k: OpenAIModelConfig,
        ModelType.GOOGLE_BARD: GoogleBardModelConfig,
    }

    @classmethod
    def get(cls, model_type: ModelType, **kwargs) -> BaseModelConfig:
        if model_type not in cls._instances:
            if model_type not in cls._config_class_mapping:
                raise ValueError(f"Invalid model_type {model_type}")
            cls._instances[model_type] = cls._config_class_mapping[model_type](**kwargs)
        return cls._instances[model_type]

    @classmethod
    def reset(cls, model_type: ModelType = None, **kwargs) -> BaseModelConfig | None:
        if model_type is None:
            cls._instances.clear()
            return None
        elif model_type not in cls._config_class_mapping:
            raise ValueError(f"Invalid model_type {model_type}")
        cls._instances[model_type] = cls._config_class_mapping[model_type](**kwargs)
        return cls._instances[model_type]

